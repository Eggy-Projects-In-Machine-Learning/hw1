{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to develop a model that predicts the composer of a piece of classical music, represented in CSV format that was converted directly from MIDI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the categories in the original csv is as follows: tick, type, time, meta, track, numerator, denominator, clocks_per_click, notated_32nd_notes_per_beat, key, tempo, control, value, channel, program, note, velocity\n",
    "\n",
    "--------------\n",
    "\n",
    "PREPROCESSING 1:\n",
    " * NOTE: time column represents the time to wait since the last note played on that channel. This information is fully duplicated in the form of a linear timeline represented by the tick column\n",
    " * NOTE: note_off and note_on + velocity of 0 are equivalent\n",
    "\n",
    "The only useful ones for us are: tick (temporal dimension), type (note_on and note_off are the only one that matters), channel, note (pitch), velocity (loudness)\n",
    "\n",
    "--------------\n",
    "\n",
    "PROPROCESSING 2:\n",
    "For training, the notes will be represented as follows:\n",
    " * a note-based DS will be generated. Each row is a note containing its start-tick, duration, pitch and velocity. Shape: (NOTES, 4)\n",
    " * a chord-based DS will then be generated. All notes that have the same start-tick, duration and velocity will be grouped together, up to 4. Shape: (CHORDS, 4, 4). Zeros will be used as padding for chords less than 4 notes.\n",
    "\n",
    "Each song piece may have a maximum CHORDS length of 300. This is just so we have consistent training and testing data shape, in reality it should theoritically work for other sizes as well.\n",
    "\n",
    "Both will be attempted for training, I suspect the chord based method might have advantages but I'm not sure.\n",
    "\n",
    "--------------\n",
    "\n",
    "DATA AUGMENTATION: we can segment the data in many more ways, eg chords 0-300, 10-310, ... , 5900-6200 etc\n",
    "\n",
    "--------------\n",
    "\n",
    "DATA EXPLORATION:\n",
    " * statistics on chord formation, number of notes sharing the same tick, overall velocity, duration and pitch running average/std deviation\n",
    " * outliers in terms of song length, too many notes per chord, long pauses between notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_array import Dict_Array\n",
    "\n",
    "def process_notes(notes):\n",
    "    FEATURES_PER_CHORD = 12\n",
    "    pending_notes = {}\n",
    "    processed_notes = Dict_Array(list_type='list')\n",
    "\n",
    "    #print(f'notes: {notes}')\n",
    "\n",
    "    for note in notes:\n",
    "        if note['type'] not in ('note_on', 'note_off'):\n",
    "            continue\n",
    "        note_id = f\"{note['note']}+{note['channel']}\"\n",
    "        if note_id in pending_notes and (note['velocity'] == 0 or note['type'] == 'note_off'):\n",
    "            pn = pending_notes[note_id]\n",
    "            processed_note = {'tick': pn['tick'], 'duration': note['tick'] - pn['tick'], 'note': pn['note'], 'channel': pn['channel'], 'velocity': pn['velocity']}\n",
    "            processed_notes.add(f\"{note['tick']}+{note['channel']}+{processed_note['duration']}\", processed_note)\n",
    "        elif note['type'] == 'note_on' and note['velocity'] > 0:\n",
    "            pending_notes.update({note_id:note})\n",
    "\n",
    "    #print(f'processed notes: {processed_notes.dictionary}')\n",
    "    # at this point, all chords are organized together within processed_notes, they just require sorting before insertion into chords\n",
    "    sorted_keys = list(processed_notes.dictionary.keys())\n",
    "    sorted_keys.sort()\n",
    "    chords = np.zeros((len(sorted_keys), FEATURES_PER_CHORD))\n",
    "\n",
    "    for i, key in enumerate(sorted_keys):\n",
    "        notes = processed_notes.get(key)\n",
    "        for j, note in enumerate(notes):\n",
    "            if j == 0:\n",
    "                chords[i, 0] = note['tick']\n",
    "                chords[i, 1] = note['channel']\n",
    "                chords[i, 2] = note['duration']\n",
    "                chords[i, 3] = note['velocity']\n",
    "            if 4 + j == FEATURES_PER_CHORD:\n",
    "                break\n",
    "            chords[i, 4 + j] = note['note']\n",
    "\n",
    "    #print(f'chords: {chords}')\n",
    "\n",
    "    return chords\n",
    "    \n",
    "\n",
    "# augment into multiple chord sets\n",
    "def split_chords_into_sets(chords):\n",
    "    FEATURES_PER_CHORD = len(chords[0])\n",
    "    SKIP_SIZE = 40\n",
    "    CHORD_SET_SIZE = 4\n",
    "    start_index = 0\n",
    "    num_chord_sets = int(max(1, np.ceil((len(chords) - CHORD_SET_SIZE) / SKIP_SIZE)) + 1)\n",
    "    chord_sets = []\n",
    "    for i in range(num_chord_sets):\n",
    "        end_index = min(start_index + CHORD_SET_SIZE, len(chords))\n",
    "        if end_index <= start_index:\n",
    "            break\n",
    "\n",
    "        chord_set = np.zeros((CHORD_SET_SIZE, FEATURES_PER_CHORD))\n",
    "        chord_set[0:end_index - start_index] = chords[start_index:end_index]\n",
    "        chord_sets.append(chord_set)\n",
    "\n",
    "        start_index += SKIP_SIZE\n",
    "\n",
    "    #print(f'chord_sets: {chord_sets}')\n",
    "\n",
    "    return chord_sets\n",
    "\n",
    "\n",
    "def to_int(x):\n",
    "    if x is None or x == '':\n",
    "        x = -1\n",
    "    return int(x)\n",
    "\n",
    "\n",
    "def import_data(data_dir, folders):\n",
    "    # folders respresent the directories to import, leave as none to import all\n",
    "    CATEGORIES = len(folders)\n",
    "\n",
    "    total_items = 0\n",
    "    categories = 0\n",
    "    for subdir in os.listdir(data_dir):\n",
    "        if folders is not None and subdir not in folders:\n",
    "            continue\n",
    "        subdir_path = os.path.join(data_dir, subdir)\n",
    "        if not os.path.isdir(subdir_path):\n",
    "            continue\n",
    "        categories += 1\n",
    "        total_items += len(os.listdir(subdir_path))\n",
    "\n",
    "    X_temp = []\n",
    "    Y_temp = []\n",
    "\n",
    "    chords = []\n",
    "    \n",
    "    category_counter = 0\n",
    "    item_counter = 0\n",
    "    # List all subfolders\n",
    "    for subdir in os.listdir(data_dir):\n",
    "        # if folders list parameter is specified, only continue if this directory is included in it\n",
    "        if folders is not None and subdir not in folders:\n",
    "            continue\n",
    "        subdir_path = os.path.join(data_dir, subdir)\n",
    "        # ensure this subdirectory is a folder\n",
    "        if not os.path.isdir(subdir_path):\n",
    "            continue\n",
    "        print(f'reading directory {subdir}')\n",
    "\n",
    "        # process each file in the subdirectory\n",
    "        for filename in os.listdir(subdir_path):\n",
    "            file_path = os.path.join(subdir_path, filename)\n",
    "\n",
    "            all_notes = []\n",
    "\n",
    "            with open(file_path, 'r') as csvfile:\n",
    "                csvreader = csv.reader(csvfile)\n",
    "                header = next(csvreader)\n",
    "                IDX_TICK = header.index('tick')\n",
    "                IDX_TYPE = header.index('type')\n",
    "                IDX_CHAN = header.index('channel')\n",
    "                IDX_NOTE = header.index('note')\n",
    "                IDX_VELO = header.index('velocity')\n",
    "\n",
    "                for row in csvreader:\n",
    "                    all_notes.append({'tick': to_int(row[IDX_TICK]), \n",
    "                                      'type': row[IDX_TYPE], \n",
    "                                      'channel': to_int(row[IDX_CHAN]), \n",
    "                                      'note': to_int(row[IDX_NOTE]), \n",
    "                                      'velocity': to_int(row[IDX_VELO])})\n",
    "\n",
    "            chords = process_notes(all_notes)\n",
    "            chord_sets = split_chords_into_sets(chords)\n",
    "            X_temp.extend(chord_sets)\n",
    "            \n",
    "            category_one_hot = [0] * CATEGORIES\n",
    "            category_one_hot[category_counter] = 1\n",
    "            categories_extension = []\n",
    "            for _ in range(len(chord_sets)):\n",
    "                categories_extension.append(category_one_hot)\n",
    "            Y_temp.extend(categories_extension)\n",
    "\n",
    "            item_counter += 1\n",
    "\n",
    "        category_counter += 1\n",
    "\n",
    "    X = np.array(X_temp)\n",
    "    Y = np.array(Y_temp)\n",
    "    print(f'X shape: {X.shape} Y shape: {Y.shape}')\n",
    "    print(f'CATEGORY STATS:')\n",
    "    for i in range(Y.shape[1]):\n",
    "        print(f'  {i}: {np.sum(Y[:,i])}')\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VISUALIZATION:\n",
    "\n",
    "take a chordset and print it in music sheet style to see if it looks reasonable. Compare to original. This helps with outlier and anomaly detection, and catched mistakes that might have been present in the original dataset. Outliers can then be removed when necessary, such as a note that is (mistakenly) held for the entire piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading directory Chopin\n",
      "reading directory Vivaldi\n",
      "X shape: (14691, 4, 12) Y shape: (14691, 2)\n",
      "CATEGORY STATS:\n",
      "  0: 6328\n",
      "  1: 8363\n",
      "xtrain: (13221, 4, 12) ytrain: (13221, 2) xtest: (1470, 4, 12) ytest: (1470, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = '../../Data/music/midi/'\n",
    "\n",
    "FEATURES_PER_CHORD = 12\n",
    "CATEGORIES = [\"Chopin\", \"Vivaldi\"]\n",
    "n_categories = len(CATEGORIES)\n",
    "\n",
    "X, Y = import_data(data_path, CATEGORIES)\n",
    "X = X.astype(np.float32)\n",
    "Y = Y.astype(np.float32)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "print(f'xtrain: {x_train.shape} ytrain: {y_train.shape} xtest: {x_test.shape} ytest: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: make RNN model and visualization. I have preprocessed the data into an organized, uniform format ready to be used. May need to build it custom from linear\n",
    "\n",
    "def get_category(output):\n",
    "    return torch.argmax(output)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #print(f'input: {input.size()}, hidden: {hidden.size()}')\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.in2h(combined)\n",
    "        output = self.h2out(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        #return torch.zeros(1, self.hidden_size)\n",
    "        return (torch.rand(1, self.hidden_size) - 0.5) * 0.02\n",
    "    \n",
    "x_train = torch.from_numpy(x_train)\n",
    "x_train = torch.unsqueeze(x_train, 2)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_train = torch.unsqueeze(y_train, 1)\n",
    "y_train = y_train.type(torch.LongTensor)\n",
    "\n",
    "x_test = torch.from_numpy(x_test)\n",
    "x_test = torch.unsqueeze(x_test, 2)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "y_test = torch.unsqueeze(y_test, 1)\n",
    "y_test = y_test.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, iteration 661/13221, loss 0.0, 307/661 correct ( 0.46444780635400906 )\n",
      "epoch 0, iteration 1322/13221, loss 0.0, 643/1322 correct ( 0.4863842662632375 )\n",
      "epoch 0, iteration 1983/13221, loss 0.0, 957/1983 correct ( 0.4826021180030257 )\n",
      "epoch 0, iteration 2644/13221, loss 1218897920.0, 1273/2644 correct ( 0.48146747352496216 )\n",
      "epoch 0, iteration 3305/13221, loss 468072704.0, 1606/3305 correct ( 0.4859304084720121 )\n",
      "epoch 0, iteration 3966/13221, loss 1133373440.0, 1938/3966 correct ( 0.4886535552193646 )\n",
      "epoch 0, iteration 4627/13221, loss 0.0, 2285/4627 correct ( 0.4938405014047979 )\n",
      "epoch 0, iteration 5288/13221, loss 370916608.0, 2600/5288 correct ( 0.491679273827534 )\n",
      "epoch 0, iteration 5949/13221, loss 0.0, 2906/5949 correct ( 0.48848545974113294 )\n",
      "epoch 0, iteration 6610/13221, loss 0.0, 3226/6610 correct ( 0.4880484114977307 )\n",
      "epoch 0, iteration 7271/13221, loss 0.0, 3532/7271 correct ( 0.48576536927520286 )\n",
      "epoch 0, iteration 7932/13221, loss 221268704.0, 3854/7932 correct ( 0.4858799798285426 )\n",
      "epoch 0, iteration 8593/13221, loss 0.0, 4168/8593 correct ( 0.48504596764808566 )\n",
      "epoch 0, iteration 9254/13221, loss 258132288.0, 4489/9254 correct ( 0.4850875297168792 )\n",
      "epoch 0, iteration 9915/13221, loss 0.0, 4819/9915 correct ( 0.48603126575895106 )\n",
      "epoch 0, iteration 10576/13221, loss 0.0, 5148/10576 correct ( 0.4867624810892587 )\n",
      "epoch 0, iteration 11237/13221, loss 367904256.0, 5491/11237 correct ( 0.4886535552193646 )\n",
      "epoch 0, iteration 11898/13221, loss 0.0, 5813/11898 correct ( 0.4885695074802488 )\n",
      "epoch 0, iteration 12559/13221, loss 139652192.0, 6143/12559 correct ( 0.48913130026275975 )\n",
      "epoch 0, iteration 13220/13221, loss 224252704.0, 6493/13220 correct ( 0.4911497730711044 )\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[199], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m chord_set \u001b[38;5;241m=\u001b[39m x_train[i]\n\u001b[0;32m     36\u001b[0m category \u001b[38;5;241m=\u001b[39m y_train[i]\n\u001b[1;32m---> 37\u001b[0m output, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchord_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m current_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     40\u001b[0m guess \u001b[38;5;241m=\u001b[39m get_category(output)\n",
      "Cell \u001b[1;32mIn[199], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(chord_set, category)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#print(f'output: {output} category: {category}')\u001b[39;00m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output[\u001b[38;5;241m0\u001b[39m], category[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Add parameters' gradients to their values, multiplied by learning rate\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m rnn\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[1;32mc:\\Users\\eggyr\\anaconda3\\envs\\pytorch3d\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eggyr\\anaconda3\\envs\\pytorch3d\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[199], line 20\u001b[0m, in \u001b[0;36mtrain.<locals>.<lambda>\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m rnn\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[0;32m     19\u001b[0m     clip_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e2\u001b[39m\n\u001b[1;32m---> 20\u001b[0m     p\u001b[38;5;241m.\u001b[39mregister_hook(\u001b[38;5;28;01mlambda\u001b[39;00m grad: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mclip_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_value\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#print(f'p data: {p.data} grad: {p.grad.data}')\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     p\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39madd_(p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mlearning_rate)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "criterion = nn.NLLLoss()\n",
    "n_hidden = 128\n",
    "rnn = RNN(FEATURES_PER_CHORD, n_hidden, n_categories)\n",
    "\n",
    "def train(chord_set, category):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(chord_set.size()[0]):\n",
    "        output, hidden = rnn(chord_set[i], hidden)\n",
    "\n",
    "    #print(f'output: {output} category: {category}')\n",
    "    loss = criterion(output[0], category[0])\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        clip_value = 1e2\n",
    "        p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))\n",
    "        #print(f'p data: {p.data} grad: {p.grad.data}')\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "\n",
    "n_epoch = 10\n",
    "all_losses = []\n",
    "current_loss = 0\n",
    "correct = 0\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    plot_interval = max(1, int(len(x_train) / 20))\n",
    "    for i in range(len(x_train)):\n",
    "        chord_set = x_train[i]\n",
    "        category = y_train[i]\n",
    "        output, loss = train(chord_set, category)\n",
    "        current_loss += loss\n",
    "\n",
    "        guess = get_category(output)\n",
    "        if guess == torch.argmax(category):\n",
    "            correct += 1\n",
    "\n",
    "        if i % plot_interval == 0 and i != 0:\n",
    "            print(f'epoch {epoch}, iteration {i}/{len(x_train)}, loss {loss}, {correct}/{i} correct ( {correct/i} )')\n",
    "            all_losses.append(current_loss/plot_interval)\n",
    "            current_loss = 0\n",
    "\n",
    "    correct = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
