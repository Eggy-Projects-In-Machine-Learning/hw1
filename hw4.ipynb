{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to develop a model that predicts the composer of a piece of classical music, represented in CSV format that was converted directly from MIDI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the categories in the original csv is as follows: tick, type, time, meta, track, numerator, denominator, clocks_per_click, notated_32nd_notes_per_beat, key, tempo, control, value, channel, program, note, velocity\n",
    "\n",
    "--------------\n",
    "\n",
    "PREPROCESSING 1:\n",
    " * NOTE: time column represents the time to wait since the last note played on that channel. This information is fully duplicated in the form of a linear timeline represented by the tick column\n",
    " * NOTE: note_off and note_on + velocity of 0 are equivalent\n",
    "\n",
    "The only useful ones for us are: tick (temporal dimension), type (note_on and note_off are the only one that matters), channel, note (pitch), velocity (loudness)\n",
    "\n",
    "--------------\n",
    "\n",
    "PROPROCESSING 2:\n",
    "For training, the notes will be represented as follows:\n",
    " * a note-based DS will be generated. Each row is a note containing its start-tick, duration, pitch and velocity. Shape: (NOTES, 4)\n",
    " * a chord-based DS will then be generated. All notes that have the same start-tick, duration and velocity will be grouped together, up to 4. Shape: (CHORDS, 4, 4). Zeros will be used as padding for chords less than 4 notes.\n",
    "\n",
    "Each song piece may have a maximum CHORDS length of 300. This is just so we have consistent training and testing data shape, in reality it should theoritically work for other sizes as well.\n",
    "\n",
    "Both will be attempted for training, I suspect the chord based method might have advantages but I'm not sure.\n",
    "\n",
    "--------------\n",
    "\n",
    "DATA AUGMENTATION: we can segment the data in many more ways, eg chords 0-300, 10-310, ... , 5900-6200 etc\n",
    "\n",
    "--------------\n",
    "\n",
    "DATA EXPLORATION:\n",
    " * statistics on chord formation, number of notes sharing the same tick, overall velocity, duration and pitch running average/std deviation\n",
    " * outliers in terms of song length, too many notes per chord, long pauses between notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_array import Dict_Array\n",
    "\n",
    "def process_notes(notes):\n",
    "    FEATURES_PER_CHORD = 129\n",
    "    pending_notes = {}\n",
    "    processed_notes = Dict_Array(list_type='list')\n",
    "\n",
    "    #print(f'notes: {notes}')\n",
    "\n",
    "    for note in notes:\n",
    "        if note['type'] not in ('note_on', 'note_off'):\n",
    "            continue\n",
    "        note_id = f\"{note['note']}+{note['channel']}\"\n",
    "        if note_id in pending_notes and (note['velocity'] == 0 or note['type'] == 'note_off'):\n",
    "            pn = pending_notes[note_id]\n",
    "            processed_note = {'tick': pn['tick'], 'duration': note['tick'] - pn['tick'], 'note': pn['note'], 'channel': pn['channel'], 'velocity': pn['velocity']}\n",
    "            processed_notes.add(f\"{note['tick']}+{note['channel']}+{processed_note['duration']}\", processed_note)\n",
    "        elif note['type'] == 'note_on' and note['velocity'] > 0:\n",
    "            pending_notes.update({note_id:note})\n",
    "\n",
    "    \n",
    "    # at this point, all chords are organized together within processed_notes, they just require sorting before insertion into chords\n",
    "    # TODO: convert processed_notes into a key held one hot encoding format.\n",
    "    \n",
    "    sorted_keys = list(processed_notes.dictionary.keys())\n",
    "    sorted_keys.sort()\n",
    "    chords = np.zeros((len(sorted_keys), FEATURES_PER_CHORD))\n",
    "\n",
    "    for i, key in enumerate(sorted_keys):\n",
    "        notes = processed_notes.get(key)\n",
    "        for j, note in enumerate(notes):\n",
    "            chords[i, note['note'] + 1] = note['velocity'] + 100\n",
    "\n",
    "    #print(f'chords: {chords}')\n",
    "\n",
    "    return chords\n",
    "    \n",
    "\n",
    "# augment into multiple chord sets\n",
    "def split_chords_into_sets(chords):\n",
    "    FEATURES_PER_CHORD = len(chords[0])\n",
    "    SKIP_SIZE = 100\n",
    "    CHORD_SET_SIZE = 25\n",
    "    start_index = 0\n",
    "    num_chord_sets = int(max(1, np.ceil((len(chords) - CHORD_SET_SIZE) / SKIP_SIZE)) + 1)\n",
    "    chord_sets = []\n",
    "    for i in range(num_chord_sets):\n",
    "        end_index = min(start_index + CHORD_SET_SIZE, len(chords))\n",
    "        if end_index <= start_index:\n",
    "            break\n",
    "\n",
    "        chord_set = np.zeros((CHORD_SET_SIZE, FEATURES_PER_CHORD))\n",
    "        chord_set[0:end_index - start_index] = chords[start_index:end_index]\n",
    "        chord_sets.append(chord_set)\n",
    "\n",
    "        start_index += SKIP_SIZE\n",
    "\n",
    "    #print(f'chord_sets: {chord_sets}')\n",
    "\n",
    "    return chord_sets\n",
    "\n",
    "\n",
    "def to_int(x):\n",
    "    if x is None or x == '':\n",
    "        x = -1\n",
    "    return int(x)\n",
    "\n",
    "\n",
    "def import_data(data_dir, folders):\n",
    "    # folders respresent the directories to import, leave as none to import all\n",
    "    CATEGORIES = len(folders)\n",
    "\n",
    "    total_items = 0\n",
    "    categories = 0\n",
    "    for subdir in os.listdir(data_dir):\n",
    "        if folders is not None and subdir not in folders:\n",
    "            continue\n",
    "        subdir_path = os.path.join(data_dir, subdir)\n",
    "        if not os.path.isdir(subdir_path):\n",
    "            continue\n",
    "        categories += 1\n",
    "        total_items += len(os.listdir(subdir_path))\n",
    "\n",
    "    X_temp = []\n",
    "    Y_temp = []\n",
    "\n",
    "    chords = []\n",
    "    \n",
    "    category_counter = 0\n",
    "    item_counter = 0\n",
    "    # List all subfolders\n",
    "    for subdir in os.listdir(data_dir):\n",
    "        # if folders list parameter is specified, only continue if this directory is included in it\n",
    "        if folders is not None and subdir not in folders:\n",
    "            continue\n",
    "        subdir_path = os.path.join(data_dir, subdir)\n",
    "        # ensure this subdirectory is a folder\n",
    "        if not os.path.isdir(subdir_path):\n",
    "            continue\n",
    "        print(f'reading directory {subdir}')\n",
    "\n",
    "        # process each file in the subdirectory\n",
    "        for filename in os.listdir(subdir_path):\n",
    "            file_path = os.path.join(subdir_path, filename)\n",
    "\n",
    "            all_notes = []\n",
    "\n",
    "            with open(file_path, 'r') as csvfile:\n",
    "                csvreader = csv.reader(csvfile)\n",
    "                header = next(csvreader)\n",
    "                IDX_TICK = header.index('tick')\n",
    "                IDX_TYPE = header.index('type')\n",
    "                IDX_CHAN = header.index('channel')\n",
    "                IDX_NOTE = header.index('note')\n",
    "                IDX_VELO = header.index('velocity')\n",
    "\n",
    "                for row in csvreader:\n",
    "                    all_notes.append({'tick': to_int(row[IDX_TICK]), \n",
    "                                      'type': row[IDX_TYPE], \n",
    "                                      'channel': to_int(row[IDX_CHAN]), \n",
    "                                      'note': to_int(row[IDX_NOTE]), \n",
    "                                      'velocity': to_int(row[IDX_VELO])})\n",
    "\n",
    "            chords = process_notes(all_notes)\n",
    "            chord_sets = split_chords_into_sets(chords)\n",
    "            X_temp.extend(chord_sets)\n",
    "            \n",
    "            category_one_hot = [0] * CATEGORIES\n",
    "            category_one_hot[category_counter] = 1\n",
    "            categories_extension = []\n",
    "            for _ in range(len(chord_sets)):\n",
    "                categories_extension.append(category_one_hot)\n",
    "            Y_temp.extend(categories_extension)\n",
    "\n",
    "            item_counter += 1\n",
    "\n",
    "        category_counter += 1\n",
    "\n",
    "    X = np.array(X_temp)\n",
    "    Y = np.array(Y_temp)\n",
    "    print(f'X shape: {X.shape} Y shape: {Y.shape}')\n",
    "    print(f'CATEGORY STATS:')\n",
    "    for i in range(Y.shape[1]):\n",
    "        print(f'  {i}: {np.sum(Y[:,i])}')\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VISUALIZATION:\n",
    "\n",
    "take a chordset and print it in music sheet style to see if it looks reasonable. Compare to original. This helps with outlier and anomaly detection, and catched mistakes that might have been present in the original dataset. Outliers can then be removed when necessary, such as a note that is (mistakenly) held for the entire piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading directory Chopin\n",
      "reading directory Vivaldi\n",
      "X shape: (5944, 25, 129) Y shape: (5944, 2)\n",
      "CATEGORY STATS:\n",
      "  0: 2571\n",
      "  1: 3373\n",
      "xtrain: (5349, 25, 129) ytrain: (5349, 2) xtest: (595, 25, 129) ytest: (595, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = '../../Data/music/midi/'\n",
    "\n",
    "FEATURES_PER_CHORD = 129\n",
    "CATEGORIES = ['Chopin', 'Vivaldi']\n",
    "#CATEGORIES = [\"Chopin\", \"Vivaldi\", \"Scarlatti\", \"Haendel\"]\n",
    "n_categories = len(CATEGORIES)\n",
    "\n",
    "X, Y = import_data(data_path, CATEGORIES)\n",
    "X = X.astype(np.float32)\n",
    "Y = Y.astype(np.float32)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "print(f'xtrain: {x_train.shape} ytrain: {y_train.shape} xtest: {x_test.shape} ytest: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: make RNN model and visualization. I have preprocessed the data into an organized, uniform format ready to be used. May need to build it custom from linear\n",
    "\n",
    "def get_category(output):\n",
    "    return torch.argmax(torch.abs(output))\n",
    "    #return torch.argmax(output)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #print(f'input: {input.size()}, hidden: {hidden.size()}')\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.in2h(combined)\n",
    "        output = self.h2out(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        #return torch.zeros(1, self.hidden_size)\n",
    "        return (torch.rand(1, self.hidden_size) - 0.5) * 0.02\n",
    "    \n",
    "x_train = torch.from_numpy(x_train)\n",
    "x_train = torch.unsqueeze(x_train, 2)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_train = torch.unsqueeze(y_train, 1)\n",
    "y_train = y_train.type(torch.LongTensor)\n",
    "\n",
    "x_test = torch.from_numpy(x_test)\n",
    "x_test = torch.unsqueeze(x_test, 2)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "y_test = torch.unsqueeze(y_test, 1)\n",
    "y_test = y_test.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, iteration 267/5349, loss 2.684558664530926, 137/268 correct ( 0.5111940298507462 )\n",
      "testing accuracy: 0.5394957983193277\n",
      "epoch 0, iteration 534/5349, loss 2.1844922330347876, 158/267 correct ( 0.5917602996254682 )\n",
      "testing accuracy: 0.5596638655462185\n",
      "epoch 0, iteration 801/5349, loss 1.9355258952087893, 166/267 correct ( 0.6217228464419475 )\n",
      "testing accuracy: 0.5915966386554622\n",
      "epoch 0, iteration 1068/5349, loss 2.062102575800115, 145/267 correct ( 0.5430711610486891 )\n",
      "testing accuracy: 0.6067226890756302\n",
      "epoch 0, iteration 1335/5349, loss 1.4682692638281007, 185/267 correct ( 0.6928838951310862 )\n",
      "testing accuracy: 0.6235294117647059\n",
      "epoch 0, iteration 1602/5349, loss 1.7341596187513861, 153/267 correct ( 0.5730337078651685 )\n",
      "testing accuracy: 0.6470588235294118\n",
      "epoch 0, iteration 1869/5349, loss 1.6215140293884756, 165/267 correct ( 0.6179775280898876 )\n",
      "testing accuracy: 0.6470588235294118\n",
      "epoch 0, iteration 2136/5349, loss 1.8221420661626293, 156/267 correct ( 0.5842696629213483 )\n",
      "testing accuracy: 0.6605042016806723\n",
      "epoch 0, iteration 2403/5349, loss 1.4810426622365638, 183/267 correct ( 0.6853932584269663 )\n",
      "testing accuracy: 0.6571428571428571\n",
      "epoch 0, iteration 2670/5349, loss 1.2860618175744556, 180/267 correct ( 0.6741573033707865 )\n",
      "testing accuracy: 0.6588235294117647\n",
      "epoch 0, iteration 2937/5349, loss 1.6074039781349656, 183/267 correct ( 0.6853932584269663 )\n",
      "testing accuracy: 0.6487394957983194\n",
      "epoch 0, iteration 3204/5349, loss 1.5355677483137762, 160/267 correct ( 0.599250936329588 )\n",
      "testing accuracy: 0.6722689075630253\n",
      "epoch 0, iteration 3471/5349, loss 1.5041782149714757, 177/267 correct ( 0.6629213483146067 )\n",
      "testing accuracy: 0.6638655462184874\n",
      "epoch 0, iteration 3738/5349, loss 1.3234333664589402, 188/267 correct ( 0.704119850187266 )\n",
      "testing accuracy: 0.6941176470588235\n",
      "epoch 0, iteration 4005/5349, loss 1.3059682472221827, 188/267 correct ( 0.704119850187266 )\n",
      "testing accuracy: 0.6991596638655462\n",
      "epoch 0, iteration 4272/5349, loss 1.08193836021365, 188/267 correct ( 0.704119850187266 )\n",
      "testing accuracy: 0.680672268907563\n",
      "epoch 0, iteration 4539/5349, loss 1.3634730372787662, 174/267 correct ( 0.651685393258427 )\n",
      "testing accuracy: 0.6840336134453782\n",
      "epoch 0, iteration 4806/5349, loss 1.1578660817528945, 192/267 correct ( 0.7191011235955056 )\n",
      "testing accuracy: 0.6991596638655462\n",
      "epoch 0, iteration 5073/5349, loss 1.12922706993307, 195/267 correct ( 0.7303370786516854 )\n",
      "testing accuracy: 0.7025210084033613\n",
      "epoch 0, iteration 5340/5349, loss 1.4536516011141056, 180/267 correct ( 0.6741573033707865 )\n",
      "testing accuracy: 0.6941176470588235\n",
      "epoch 1, iteration 267/5349, loss 1.4056185817083053, 185/276 correct ( 0.6702898550724637 )\n",
      "testing accuracy: 0.6840336134453782\n",
      "epoch 1, iteration 534/5349, loss 0.966039225860211, 196/267 correct ( 0.7340823970037453 )\n",
      "testing accuracy: 0.6756302521008404\n",
      "epoch 1, iteration 801/5349, loss 1.2369675194529992, 193/267 correct ( 0.7228464419475655 )\n",
      "testing accuracy: 0.6890756302521008\n",
      "epoch 1, iteration 1068/5349, loss 1.4010712190454166, 186/267 correct ( 0.6966292134831461 )\n",
      "testing accuracy: 0.6907563025210084\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m chord_set \u001b[38;5;241m=\u001b[39m x_train[i]\n\u001b[0;32m     48\u001b[0m category \u001b[38;5;241m=\u001b[39m y_train[i]\n\u001b[1;32m---> 49\u001b[0m output, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchord_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m current_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m/\u001b[39mplot_interval\n\u001b[0;32m     52\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(chord_set, category)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#print(f'output: {output} category: {category}')\u001b[39;00m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output[\u001b[38;5;241m0\u001b[39m], category[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Add parameters' gradients to their values, multiplied by learning rate\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m rnn\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[1;32mc:\\Users\\eggyr\\anaconda3\\envs\\pytorch3d\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eggyr\\anaconda3\\envs\\pytorch3d\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m, in \u001b[0;36mtrain.<locals>.<lambda>\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m rnn\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[0;32m     19\u001b[0m     clip_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m---> 20\u001b[0m     p\u001b[38;5;241m.\u001b[39mregister_hook(\u001b[38;5;28;01mlambda\u001b[39;00m grad: torch\u001b[38;5;241m.\u001b[39mclamp(grad, \u001b[38;5;241m-\u001b[39mclip_value, clip_value))\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#print(f'p data: {p.data} grad: {p.grad.data}')\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     p\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39madd_(p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mlearning_rate)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "criterion = nn.NLLLoss()\n",
    "n_hidden = 128\n",
    "rnn = RNN(FEATURES_PER_CHORD, n_hidden, n_categories)\n",
    "\n",
    "def train(chord_set, category):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(chord_set.size()[0]):\n",
    "        output, hidden = rnn(chord_set[i], hidden)\n",
    "\n",
    "    #print(f'output: {output} category: {category}')\n",
    "    loss = criterion(output[0], category[0])\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        clip_value = 0.1\n",
    "        p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))\n",
    "        #print(f'p data: {p.data} grad: {p.grad.data}')\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "\n",
    "def test(testX, testY):\n",
    "    correct = 0\n",
    "    for index, x in enumerate(testX):\n",
    "        hidden = rnn.initHidden()\n",
    "        for i in range(chord_set.size()[0]):\n",
    "            output, hidden = rnn(x[i], hidden)\n",
    "        if get_category(output) == get_category(testY[index]):\n",
    "            correct += 1\n",
    "    return correct / len(testX)\n",
    "\n",
    "\n",
    "n_epoch = 10\n",
    "all_losses = []\n",
    "current_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    plot_interval = max(1, int(len(x_train) / 20))\n",
    "    for i in range(len(x_train)):\n",
    "        chord_set = x_train[i]\n",
    "        category = y_train[i]\n",
    "        output, loss = train(chord_set, category)\n",
    "        current_loss += loss/plot_interval\n",
    "\n",
    "        total += 1\n",
    "        if get_category(output) == torch.argmax(category):\n",
    "            #print(f'carrot: OUTPUT {torch.argmax(output)}')\n",
    "            correct += 1\n",
    "        #else:\n",
    "           # print(f'INCORRECT LMAO, output: {output} answer: {category}')\n",
    "\n",
    "        if i % plot_interval == 0 and i != 0:\n",
    "            print(f'epoch {epoch}, iteration {i}/{len(x_train)}, loss {current_loss}, {correct}/{total} correct ( {correct/total} )')\n",
    "            all_losses.append(current_loss)\n",
    "            current_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            print(f'testing accuracy: {test(x_test, y_test)}')\n",
    "\n",
    "    correct = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
