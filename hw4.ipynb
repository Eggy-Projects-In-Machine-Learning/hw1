{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to develop a model that predicts the composer of a piece of classical music, represented in CSV format that was converted directly from MIDI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the categories in the original csv is as follows: tick, type, time, meta, track, numerator, denominator, clocks_per_click, notated_32nd_notes_per_beat, key, tempo, control, value, channel, program, note, velocity\n",
    "\n",
    "--------------\n",
    "\n",
    "PREPROCESSING 1:\n",
    " * NOTE: time column represents the time to wait since the last note played on that channel. This information is fully duplicated in the form of a linear timeline represented by the tick column\n",
    " * NOTE: note_off and note_on + velocity of 0 are equivalent\n",
    "\n",
    "The only useful ones for us are: tick (temporal dimension), type (note_on and note_off are the only one that matters), channel, note (pitch), velocity (loudness)\n",
    "\n",
    "--------------\n",
    "\n",
    "PROPROCESSING 2:\n",
    "For training, the notes will be represented as follows:\n",
    " * a note-based DS will be generated. Each row is a note containing its start-tick, duration, pitch and velocity. Shape: (NOTES, 4)\n",
    " * a chord-based DS will then be generated. All notes that have the same start-tick, duration and velocity will be grouped together, up to 4. Shape: (CHORDS, 4, 4). Zeros will be used as padding for chords less than 4 notes.\n",
    "\n",
    "Each song piece may have a maximum CHORDS length of 300. This is just so we have consistent training and testing data shape, in reality it should theoritically work for other sizes as well.\n",
    "\n",
    "Both will be attempted for training, I suspect the chord based method might have advantages but I'm not sure.\n",
    "\n",
    "--------------\n",
    "\n",
    "DATA AUGMENTATION: we can segment the data in many more ways, eg chords 0-300, 10-310, ... , 5900-6200 etc\n",
    "\n",
    "--------------\n",
    "\n",
    "DATA EXPLORATION:\n",
    " * statistics on chord formation, number of notes sharing the same tick, overall velocity, duration and pitch running average/std deviation\n",
    " * outliers in terms of song length, too many notes per chord, long pauses between notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_array import Dict_Array\n",
    "\n",
    "def process_notes(notes):\n",
    "    FEATURES_PER_CHORD = 129\n",
    "    pending_notes = {}\n",
    "    processed_notes = Dict_Array(list_type='list')\n",
    "\n",
    "    #print(f'notes: {notes}')\n",
    "\n",
    "    for note in notes:\n",
    "        if note['type'] not in ('note_on', 'note_off'):\n",
    "            continue\n",
    "        note_id = f\"{note['note']}+{note['channel']}\"\n",
    "        if note_id in pending_notes and (note['velocity'] == 0 or note['type'] == 'note_off'):\n",
    "            pn = pending_notes[note_id]\n",
    "            processed_note = {'tick': pn['tick'], 'duration': note['tick'] - pn['tick'], 'note': pn['note'], 'channel': pn['channel'], 'velocity': pn['velocity']}\n",
    "            processed_notes.add(f\"{note['tick']}+{note['channel']}+{processed_note['duration']}\", processed_note)\n",
    "        elif note['type'] == 'note_on' and note['velocity'] > 0:\n",
    "            pending_notes.update({note_id:note})\n",
    "\n",
    "    \n",
    "    # at this point, all chords are organized together within processed_notes, they just require sorting before insertion into chords\n",
    "    # TODO: convert processed_notes into a key held one hot encoding format.\n",
    "    \n",
    "    sorted_keys = list(processed_notes.dictionary.keys())\n",
    "    sorted_keys.sort()\n",
    "    chords = np.zeros((len(sorted_keys), FEATURES_PER_CHORD))\n",
    "\n",
    "    for i, key in enumerate(sorted_keys):\n",
    "        notes = processed_notes.get(key)\n",
    "        for j, note in enumerate(notes):\n",
    "            chords[i, note['note'] + 1] = note['velocity'] + 100\n",
    "\n",
    "    #print(f'chords: {chords}')\n",
    "\n",
    "    return chords\n",
    "    \n",
    "\n",
    "# augment into multiple chord sets\n",
    "def split_chords_into_sets(chords):\n",
    "    FEATURES_PER_CHORD = len(chords[0])\n",
    "    SKIP_SIZE = 100\n",
    "    CHORD_SET_SIZE = 25\n",
    "    start_index = 0\n",
    "    num_chord_sets = int(max(1, np.ceil((len(chords) - CHORD_SET_SIZE) / SKIP_SIZE)) + 1)\n",
    "    chord_sets = []\n",
    "    for i in range(num_chord_sets):\n",
    "        end_index = min(start_index + CHORD_SET_SIZE, len(chords))\n",
    "        if end_index <= start_index:\n",
    "            break\n",
    "\n",
    "        chord_set = np.zeros((CHORD_SET_SIZE, FEATURES_PER_CHORD))\n",
    "        chord_set[0:end_index - start_index] = chords[start_index:end_index]\n",
    "        chord_sets.append(chord_set)\n",
    "\n",
    "        start_index += SKIP_SIZE\n",
    "\n",
    "    #print(f'chord_sets: {chord_sets}')\n",
    "\n",
    "    return chord_sets\n",
    "\n",
    "\n",
    "def to_int(x):\n",
    "    if x is None or x == '':\n",
    "        x = -1\n",
    "    return int(x)\n",
    "\n",
    "\n",
    "def import_data(data_dir, folders):\n",
    "    # folders respresent the directories to import, leave as none to import all\n",
    "    CATEGORIES = len(folders)\n",
    "\n",
    "    total_items = 0\n",
    "    categories = 0\n",
    "    for subdir in os.listdir(data_dir):\n",
    "        if folders is not None and subdir not in folders:\n",
    "            continue\n",
    "        subdir_path = os.path.join(data_dir, subdir)\n",
    "        if not os.path.isdir(subdir_path):\n",
    "            continue\n",
    "        categories += 1\n",
    "        total_items += len(os.listdir(subdir_path))\n",
    "\n",
    "    X_temp = []\n",
    "    Y_temp = []\n",
    "\n",
    "    chords = []\n",
    "    \n",
    "    category_counter = 0\n",
    "    item_counter = 0\n",
    "    # List all subfolders\n",
    "    for subdir in os.listdir(data_dir):\n",
    "        # if folders list parameter is specified, only continue if this directory is included in it\n",
    "        if folders is not None and subdir not in folders:\n",
    "            continue\n",
    "        subdir_path = os.path.join(data_dir, subdir)\n",
    "        # ensure this subdirectory is a folder\n",
    "        if not os.path.isdir(subdir_path):\n",
    "            continue\n",
    "        print(f'reading directory {subdir}')\n",
    "\n",
    "        # process each file in the subdirectory\n",
    "        for filename in os.listdir(subdir_path):\n",
    "            file_path = os.path.join(subdir_path, filename)\n",
    "\n",
    "            all_notes = []\n",
    "\n",
    "            with open(file_path, 'r') as csvfile:\n",
    "                csvreader = csv.reader(csvfile)\n",
    "                header = next(csvreader)\n",
    "                IDX_TICK = header.index('tick')\n",
    "                IDX_TYPE = header.index('type')\n",
    "                IDX_CHAN = header.index('channel')\n",
    "                IDX_NOTE = header.index('note')\n",
    "                IDX_VELO = header.index('velocity')\n",
    "\n",
    "                for row in csvreader:\n",
    "                    all_notes.append({'tick': to_int(row[IDX_TICK]), \n",
    "                                      'type': row[IDX_TYPE], \n",
    "                                      'channel': to_int(row[IDX_CHAN]), \n",
    "                                      'note': to_int(row[IDX_NOTE]), \n",
    "                                      'velocity': to_int(row[IDX_VELO])})\n",
    "\n",
    "            chords = process_notes(all_notes)\n",
    "            chord_sets = split_chords_into_sets(chords)\n",
    "            X_temp.extend(chord_sets)\n",
    "            \n",
    "            category_one_hot = [0] * CATEGORIES\n",
    "            category_one_hot[category_counter] = 1\n",
    "            categories_extension = []\n",
    "            for _ in range(len(chord_sets)):\n",
    "                categories_extension.append(category_one_hot)\n",
    "            Y_temp.extend(categories_extension)\n",
    "\n",
    "            item_counter += 1\n",
    "\n",
    "        category_counter += 1\n",
    "\n",
    "    X = np.array(X_temp)\n",
    "    Y = np.array(Y_temp)\n",
    "    print(f'X shape: {X.shape} Y shape: {Y.shape}')\n",
    "    print(f'CATEGORY STATS:')\n",
    "    for i in range(Y.shape[1]):\n",
    "        print(f'  {i}: {np.sum(Y[:,i])}')\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VISUALIZATION:\n",
    "\n",
    "take a chordset and print it in music sheet style to see if it looks reasonable. Compare to original. This helps with outlier and anomaly detection, and catched mistakes that might have been present in the original dataset. Outliers can then be removed when necessary, such as a note that is (mistakenly) held for the entire piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading directory Chopin\n",
      "reading directory Vivaldi\n",
      "X shape: (5944, 25, 129) Y shape: (5944, 2)\n",
      "CATEGORY STATS:\n",
      "  0: 2571\n",
      "  1: 3373\n",
      "xtrain: (5349, 25, 129) ytrain: (5349, 2) xtest: (595, 25, 129) ytest: (595, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = '../../Data/music/midi/'\n",
    "\n",
    "FEATURES_PER_CHORD = 129\n",
    "CATEGORIES = ['Chopin', 'Vivaldi']\n",
    "#CATEGORIES = [\"Chopin\", \"Vivaldi\", \"Scarlatti\", \"Haendel\"]\n",
    "n_categories = len(CATEGORIES)\n",
    "\n",
    "X, Y = import_data(data_path, CATEGORIES)\n",
    "X = X.astype(np.float32)\n",
    "Y = Y.astype(np.float32)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "print(f'xtrain: {x_train.shape} ytrain: {y_train.shape} xtest: {x_test.shape} ytest: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: make RNN model and visualization. I have preprocessed the data into an organized, uniform format ready to be used. May need to build it custom from linear\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_category(output):\n",
    "    return torch.argmax(torch.abs(output))\n",
    "    #return torch.argmax(output)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in2h = nn.Linear(input_size + hidden_size, hidden_size, device=device)\n",
    "        self.h2out = nn.Linear(hidden_size, output_size, device=device)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #print(f'input: {input.size()}, hidden: {hidden.size()}')\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        #print(f'combined: {combined.device} input: {input.device} hidden: {hidden.device}')\n",
    "        hidden = self.in2h(combined)\n",
    "        output = self.h2out(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        #return torch.zeros(1, self.hidden_size)\n",
    "        return ((torch.rand(1, self.hidden_size, device=device) - 0.5) * 0.02)\n",
    "    \n",
    "x_train = torch.from_numpy(x_train)\n",
    "x_train = torch.unsqueeze(x_train, 2).to(device)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_train = torch.unsqueeze(y_train, 1)\n",
    "y_train = y_train.type(torch.LongTensor).to(device)\n",
    "\n",
    "x_test = torch.from_numpy(x_test)\n",
    "x_test = torch.unsqueeze(x_test, 2).to(device)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "y_test = torch.unsqueeze(y_test, 1)\n",
    "y_test = y_test.type(torch.LongTensor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, iteration 267/5349, loss 1.2525501984128382, 167/268 correct ( 0.6231343283582089 )\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m         correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     64\u001b[0m         total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 65\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest(x_test,\u001b[38;5;250m \u001b[39my_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     67\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[17], line 32\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(testX, testY)\u001b[0m\n\u001b[0;32m     30\u001b[0m hidden \u001b[38;5;241m=\u001b[39m rnn\u001b[38;5;241m.\u001b[39minitHidden()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(chord_set\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m---> 32\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_category(output) \u001b[38;5;241m==\u001b[39m get_category(testY[index]):\n\u001b[0;32m     34\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\eggyr\\anaconda3\\envs\\pytorch3d\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[16], line 22\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, input, hidden)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#print(f'combined: {combined.device} input: {input.device} hidden: {hidden.device}')\u001b[39;00m\n\u001b[0;32m     21\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min2h(combined)\n\u001b[1;32m---> 22\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh2out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(output)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, hidden\n",
      "File \u001b[1;32mc:\\Users\\eggyr\\anaconda3\\envs\\pytorch3d\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\eggyr\\anaconda3\\envs\\pytorch3d\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0005\n",
    "criterion = nn.NLLLoss()\n",
    "n_hidden = 1024\n",
    "rnn = RNN(FEATURES_PER_CHORD, n_hidden, n_categories).to(device)\n",
    "\n",
    "def train(chord_set, category):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(chord_set.size()[0]):\n",
    "        output, hidden = rnn(chord_set[i], hidden)\n",
    "\n",
    "    #print(f'output: {output} category: {category}')\n",
    "    loss = criterion(output[0], category[0])\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        clip_value = 0.1\n",
    "        p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))\n",
    "        #print(f'p data: {p.data} grad: {p.grad.data}')\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "\n",
    "def test(testX, testY):\n",
    "    correct = 0\n",
    "    for index, x in enumerate(testX):\n",
    "        hidden = rnn.initHidden()\n",
    "        for i in range(chord_set.size()[0]):\n",
    "            output, hidden = rnn(x[i], hidden)\n",
    "        if get_category(output) == get_category(testY[index]):\n",
    "            correct += 1\n",
    "    return correct / len(testX)\n",
    "\n",
    "\n",
    "n_epoch = 10\n",
    "all_losses = []\n",
    "current_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    plot_interval = max(1, int(len(x_train) / 20))\n",
    "    for i in range(len(x_train)):\n",
    "        chord_set = x_train[i]\n",
    "        category = y_train[i]\n",
    "        output, loss = train(chord_set, category)\n",
    "        current_loss += loss/plot_interval\n",
    "\n",
    "        total += 1\n",
    "        if get_category(output) == torch.argmax(category):\n",
    "            #print(f'carrot: OUTPUT {torch.argmax(output)}')\n",
    "            correct += 1\n",
    "        #else:\n",
    "           # print(f'INCORRECT LMAO, output: {output} answer: {category}')\n",
    "\n",
    "        if i % plot_interval == 0 and i != 0:\n",
    "            print(f'epoch {epoch}, iteration {i}/{len(x_train)}, loss {current_loss}, {correct}/{total} correct ( {correct/total} )')\n",
    "            all_losses.append(current_loss)\n",
    "            current_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            print(f'testing accuracy: {test(x_test, y_test)}')\n",
    "\n",
    "    correct = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
