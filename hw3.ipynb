{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this task is to predict whether it will rain or not based on Month, cloud cover, humidity, pressure, radiation, sunshine, and temperature (avg, min and max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" IMPORTING DATA \"\"\"\n",
    "\n",
    "def import_data(file_name):\n",
    "    # temporarily store data in python lists\n",
    "    data = []\n",
    "    labels = []\n",
    "    classes = 2\n",
    "\n",
    "    with open(file_name, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "\n",
    "        # skip first row because it has labels\n",
    "        next(csvreader)\n",
    "        data_indices = np.arange(1, 11)\n",
    "        data_indices = np.delete(data_indices, 5)\n",
    "        class_index = 6\n",
    "\n",
    "        for row in csvreader:\n",
    "            data.append([row[i] for i in data_indices])\n",
    "            labels.append(float(row[class_index]))\n",
    "\n",
    "    data_array = np.array(data)\n",
    "    data_array = data_array.astype(np.float32)\n",
    "    label_array = np.zeros((len(labels), classes))\n",
    "\n",
    "    # add bias\n",
    "    bias = np.ones((len(data_array), 1))\n",
    "    data_array = np.hstack((bias, data_array))\n",
    "    \n",
    "    # set one hot encoding\n",
    "    for i, label in enumerate(labels):\n",
    "        if label > 0.01:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        label_array[i][label] = 1\n",
    "\n",
    "    class_a_count = np.sum(label_array[:, 0])\n",
    "    class_b_count = np.sum(label_array[:, 1])\n",
    "    # baseline accuracy is the percentage of majority class, which is accuracy score we compare to indicate the model actually learned something useful\n",
    "    print(f'label statistics: class A {class_a_count}, class B {class_b_count}, baseline accuracy: {np.max(np.array([class_a_count, class_b_count]))/(len(data_array))}')\n",
    "    return data_array, label_array\n",
    "\n",
    "def split_data(X, Y, test_ratio):\n",
    "    num_test = int(len(X) * test_ratio)\n",
    "    test_indices = np.random.choice(len(X), num_test, replace=False)\n",
    "    Xtest = X[test_indices, :]\n",
    "    Ytest = Y[test_indices, :]\n",
    "\n",
    "    X = np.delete(X, test_indices, axis=0)\n",
    "    Y = np.delete(Y, test_indices, axis=0)\n",
    "    return X, Y, Xtest, Ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.weights = []\n",
    "\n",
    "        # initializing weights to small random values\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            self.weights.append(np.random.rand(layer_sizes[i], layer_sizes[i - 1]) * 0.1)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def d_tanh(self, x):\n",
    "        return 1 - np.power(np.tanh(x), 2)\n",
    "\n",
    "    # returns the final classification result\n",
    "    def predict(self, X):\n",
    "        A_0 = X.T\n",
    "        A_1 = self.weights[0] @ A_0\n",
    "        A_1 = self.tanh(A_1)\n",
    "        A_2 = self.weights[1] @ A_1\n",
    "        A_2 = A_2.T\n",
    "        return A_2\n",
    "    \n",
    "    # returns full history of forward so we can perform backpropagation\n",
    "    def forward(self, X):\n",
    "        A_0 = X\n",
    "        A_1 = self.weights[0] @ A_0\n",
    "        A_1 = self.tanh(A_1)\n",
    "        A_2 = self.weights[1] @ A_1\n",
    "        return (A_0, A_1, A_2)\n",
    "    \n",
    "    def backward(self, X, Y):\n",
    "        A_0 = X\n",
    "        A_0 = A_0.reshape((len(A_0), 1))\n",
    "\n",
    "        # forward propagation\n",
    "        A_0, A_1, A_2 = self.forward(A_0)\n",
    "        #print(f'shape: a0: {A_0.shape} a1: {A_1.shape} a2: {A_2.shape} y: {Y.shape}')\n",
    "\n",
    "        # backward: error of output layer\n",
    "        Y = Y.reshape((len(Y), 1))\n",
    "        dA_2 = A_2 - Y\n",
    "\n",
    "        # backward: compute weight gradient of layers\n",
    "        dW_1 = dA_2 @ A_1.T\n",
    "        dA_1 = self.weights[1].T @ dA_2 * self.d_tanh(A_1)\n",
    "        dW_0 = dA_1 @ A_0.T\n",
    "\n",
    "        # final gradient\n",
    "        weight_gradients = [dW_0, dW_1]\n",
    "        return weight_gradients\n",
    "    \n",
    "    \n",
    "    def train(self, X, Y, total_epoch, learning_rate=0.03, learning_rate_decay=0.8):\n",
    "        print(\"beginning training\")\n",
    "        for epoch in range(total_epoch):\n",
    "\n",
    "            # variable learning rate adjustment\n",
    "            # TODO: better decay algorithm, such as checking error slope\n",
    "            if epoch % (total_epoch // 10) == 0:\n",
    "                learning_rate *= learning_rate_decay\n",
    "                print(f\"epoch {epoch} with learning rate {np.around(learning_rate, 4)}\")\n",
    "\n",
    "            print(f'hstack shape: {np.hstack((X, Y)).shape}')\n",
    "            all_weight_gradients = np.apply_along_axis(lambda x: self.backward(x[:len(X[0])], x[len(X[0]):]), 1, np.hstack((X, Y)))\n",
    "            print(f'all weight gradients shape: {all_weight_gradients.shape}')\n",
    "            continue\n",
    "\n",
    "            for i in range(0, len(X)):\n",
    "\n",
    "                weight_gradients = self.backward(X[i], Y[i])\n",
    "                # subtract the gradient from the weights\n",
    "                for j in range(len(self.weights)):\n",
    "                    self.weights[j] -= learning_rate * weight_gradients[j]\n",
    "\n",
    "            if epoch % (total_epoch // 10) == 0:\n",
    "                accuracy = self.test(X, Y)\n",
    "                print(f'training accuracy: {np.around(accuracy, 5)}')\n",
    "            \n",
    "\n",
    "    def test(self, X, Y):\n",
    "        target = np.argmax(Y, axis=1)\n",
    "        predict = np.argmax(self.predict(X), axis=1)\n",
    "        num_incorrect = np.sum(target != predict)\n",
    "        accuracy = 1 - (num_incorrect/len(X))\n",
    "        self.confusion(X, Y)\n",
    "        return accuracy\n",
    "    \n",
    "    def confusion(self, X, Y):\n",
    "        confusion_matrix = np.zeros((len(Y[0]), len(Y[0])))\n",
    "        target = np.argmax(Y, axis=1)\n",
    "        predict = np.argmax(self.predict(X), axis=1)\n",
    "        for i in range(len(X)):\n",
    "            confusion_matrix[target[i], predict[i]] += 1\n",
    "        print(f'confusion matrix: \\n{confusion_matrix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label statistics: class A 2079.0, class B 1575.0, baseline accuracy: 0.5689655172413793\n",
      "Shapes: X: (2924, 10), Y: (2924, 2), Xtest: (730, 10), Ytest: (730, 2)\n",
      "beginning training\n",
      "epoch 0 with learning rate 0.006\n",
      "hstack shape: (2924, 12)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[190], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m network1\u001b[38;5;241m.\u001b[39mtest(Xtest, Ytest)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTESTING ACCURACY: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mcustom_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[190], line 11\u001b[0m, in \u001b[0;36mcustom_nn\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m network1 \u001b[38;5;241m=\u001b[39m NeuralNetwork([\u001b[38;5;28mlen\u001b[39m(Xtrain[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;241m50\u001b[39m, \u001b[38;5;28mlen\u001b[39m(Ytrain[\u001b[38;5;241m0\u001b[39m])])\n\u001b[0;32m     10\u001b[0m total_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mnetwork1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m network1\u001b[38;5;241m.\u001b[39mtest(Xtest, Ytest)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTESTING ACCURACY: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[189], line 66\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, X, Y, total_epoch, learning_rate, learning_rate_decay)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with learning rate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39maround(learning_rate,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m4\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhstack shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mhstack((X,\u001b[38;5;250m \u001b[39mY))\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m all_weight_gradients \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall weight gradients shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_weight_gradients\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eggyr\\anaconda3\\envs\\pytorch3d\\lib\\site-packages\\numpy\\lib\\shape_base.py:379\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot apply_along_axis when any iteration dimensions are 0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    378\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43minarr_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind0\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# build a buffer for storing evaluations of func1d.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# remove the requested axis, and add the new ones on the end.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# laid out so that each write is contiguous.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# for a tuple index inds, buff[inds] = func1d(inarr_view[inds])\u001b[39;00m\n\u001b[0;32m    385\u001b[0m buff \u001b[38;5;241m=\u001b[39m zeros(inarr_view\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m res\u001b[38;5;241m.\u001b[39mshape, res\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "\"\"\"TESTING\"\"\"\n",
    "\n",
    "file = '../../Data/weather/weather_prediction_dataset.csv'\n",
    "X, Y = import_data(file)\n",
    "\n",
    "def custom_nn():\n",
    "    Xtrain, Ytrain, Xtest, Ytest = split_data(X, Y, 0.2)\n",
    "    print(f'Shapes: X: {Xtrain.shape}, Y: {Ytrain.shape}, Xtest: {Xtest.shape}, Ytest: {Ytest.shape}')\n",
    "    network1 = NeuralNetwork([len(Xtrain[0]), 50, len(Ytrain[0])])\n",
    "    total_epoch = 100\n",
    "    network1.train(Xtrain, Ytrain, total_epoch, 0.01, 0.6)\n",
    "\n",
    "    accuracy = network1.test(Xtest, Ytest)\n",
    "    print(f\"TESTING ACCURACY: {accuracy}\")\n",
    "\n",
    "custom_nn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I will be implementing the same classification task but with PyTorch. I will be using a fully connected neural network with 2 hidden layers. This is done by creating an object that inherents the nn.Module object and defining a forward function. My implementation allows the neural network to be initialized with an array specifing the dimensions of each layer, with index 0 being the input and last index the output.\n",
    "\n",
    "The following resources were used to help me write this code:\n",
    "https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html (functions to call for initializing network, backpropagation, updating the weights and for inferencing)\n",
    "https://medium.com/analytics-vidhya/creating-a-custom-dataset-and-dataloader-in-pytorch-76f210a1df5d (creating dataset and dataloader objects to load my dataset)\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html (using the cross entropy loss for classification)\n",
    "\n",
    "Additionally, the train_test_split function from sklearn is used to split the data between testing and training randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, dimensions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dimensions = dimensions\n",
    "        self.layer_list = nn.ModuleList()\n",
    "        for layer_num in range(len(dimensions) - 1):\n",
    "            self.layer_list.append(nn.Linear(dimensions[layer_num], dimensions[layer_num + 1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.dimensions[0])\n",
    "        for i in range(len(self.layer_list)):\n",
    "            x = F.relu(self.layer_list[i](x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, loader, device):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    confusion = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = net(data)\n",
    "\n",
    "            output_np = output.cpu().detach().numpy()\n",
    "            answer = target.cpu().detach().numpy()\n",
    "            correct += get_num_correct(output_np, answer)\n",
    "\n",
    "            if confusion is None:\n",
    "                confusion = get_confusion_matrix(output_np, answer)\n",
    "            else:\n",
    "                confusion += get_confusion_matrix(output_np, answer)\n",
    "    \n",
    "    accuracy = (correct / len(loader.dataset))\n",
    "    print(f'confusion matrix: \\n{confusion}')\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def get_num_correct(results, Y):\n",
    "    target = np.argmax(Y, axis=1)\n",
    "    predict = np.argmax(results, axis=1)\n",
    "    num_incorrect = np.sum(target != predict)\n",
    "    return len(results) - num_incorrect\n",
    "\n",
    "\n",
    "def get_confusion_matrix(results, Y):\n",
    "    confusion_matrix = np.zeros((len(Y[0]), len(Y[0])))\n",
    "    target = np.argmax(Y, axis=1)\n",
    "    predict = np.argmax(results, axis=1)\n",
    "    for i in range(len(results)):\n",
    "        confusion_matrix[target[i], predict[i]] += 1\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "def train(net, loader, optimizer, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    net.train()\n",
    "\n",
    "    for data, target in loader:\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = net(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    accuracy = test(net, loader, device)\n",
    "    print(f'\\ttraining accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: Xtrain: (3288, 10), Ytrain: (3288, 2), Xtest: (366, 10), Ytest: (366, 2)\n",
      "Training epoch: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: \n",
      "[[1372.  501.]\n",
      " [ 352. 1063.]]\n",
      "\ttraining accuracy: 0.7405717761557178\n",
      "Training epoch: 1\n",
      "confusion matrix: \n",
      "[[1594.  279.]\n",
      " [ 677.  738.]]\n",
      "\ttraining accuracy: 0.7092457420924574\n",
      "Training epoch: 2\n",
      "confusion matrix: \n",
      "[[1301.  572.]\n",
      " [ 293. 1122.]]\n",
      "\ttraining accuracy: 0.7369221411192214\n",
      "Training epoch: 3\n",
      "confusion matrix: \n",
      "[[1379.  494.]\n",
      " [ 355. 1060.]]\n",
      "\ttraining accuracy: 0.7417883211678832\n",
      "Training epoch: 4\n",
      "confusion matrix: \n",
      "[[1242.  631.]\n",
      " [ 263. 1152.]]\n",
      "\ttraining accuracy: 0.7281021897810219\n",
      "Training epoch: 5\n",
      "confusion matrix: \n",
      "[[1428.  445.]\n",
      " [ 401. 1014.]]\n",
      "\ttraining accuracy: 0.7427007299270073\n",
      "Training epoch: 6\n",
      "confusion matrix: \n",
      "[[1293.  580.]\n",
      " [ 283. 1132.]]\n",
      "\ttraining accuracy: 0.7375304136253041\n",
      "Training epoch: 7\n",
      "confusion matrix: \n",
      "[[1467.  406.]\n",
      " [ 455.  960.]]\n",
      "\ttraining accuracy: 0.7381386861313869\n",
      "Training epoch: 8\n",
      "confusion matrix: \n",
      "[[1262.  611.]\n",
      " [ 266. 1149.]]\n",
      "\ttraining accuracy: 0.7332725060827251\n",
      "Training epoch: 9\n",
      "confusion matrix: \n",
      "[[1424.  449.]\n",
      " [ 392. 1023.]]\n",
      "\ttraining accuracy: 0.7442214111922141\n",
      "Testing\n",
      "confusion matrix: \n",
      "[[150.  56.]\n",
      " [ 40. 120.]]\n",
      "TEST ACCURACY: 0.7377049180327869\n"
     ]
    }
   ],
   "source": [
    "def torch_nn():\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "    print(f'Shapes: Xtrain: {Xtrain.shape}, Ytrain: {Ytrain.shape}, Xtest: {Xtest.shape}, Ytest: {Ytest.shape}')\n",
    "    \n",
    "    # hyperparameters\n",
    "    dimensions = [len(Xtrain[0]), 5, 5, len(Ytrain[0])]\n",
    "    train_batch_size = 10\n",
    "    test_batch_size = 10\n",
    "    total_epoch = 10\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    # PREPARING DATA\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    Xtrain = torch.tensor(Xtrain, dtype=torch.float32)\n",
    "    Ytrain = torch.tensor(Ytrain, dtype=torch.float32)\n",
    "    Xtest = torch.tensor(Xtest, dtype=torch.float32)\n",
    "    Ytest = torch.tensor(Ytest, dtype=torch.float32)\n",
    "\n",
    "    # Create Dataset objects, then create torch dataloader\n",
    "    train_dataset = SimpleDataset(Xtrain, Ytrain)\n",
    "    test_dataset = SimpleDataset(Xtest, Ytest)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    # CREATING NEURAL NETWORK OBJECT\n",
    "    network = TorchNetwork(dimensions)\n",
    "    network = network.to(device)\n",
    "    optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "    # TRAINING\n",
    "    for epoch in range(total_epoch):\n",
    "        print(f'Training epoch: {epoch}')\n",
    "        train(network, train_loader, optimizer, device)\n",
    "\n",
    "    # TESTING\n",
    "    print(\"Testing\")\n",
    "    accuracy = test(network, test_loader, device)\n",
    "    print(f'TEST ACCURACY: {accuracy}')\n",
    "    \n",
    "\n",
    "torch_nn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
